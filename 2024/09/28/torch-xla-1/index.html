<!DOCTYPE html>
<html lang=zh>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Torch XLA 入门笔记（1）—— 静态显存分配前言本来打算第二篇写LazyTensor的，近期在忙着学校的事，还有一些事情耽误了（居然真的被催更了）并且想结合dynamo一起写一篇。便把之前写了一些的显存存货给拿出来了一部分（pass和一些细节后续会再补充一下），这里只关注平台是nv gpu的情况，如有错误，请各位大佬指正 概览由于深度学习场景的特性加之XLA中对dynamic shape并">
<meta property="og:type" content="article">
<meta property="og:title" content="torch-xla-1">
<meta property="og:url" content="http://example.com/2024/09/28/torch-xla-1/index.html">
<meta property="og:site_name" content="Ronbogo&#39;s blog">
<meta property="og:description" content="Torch XLA 入门笔记（1）—— 静态显存分配前言本来打算第二篇写LazyTensor的，近期在忙着学校的事，还有一些事情耽误了（居然真的被催更了）并且想结合dynamo一起写一篇。便把之前写了一些的显存存货给拿出来了一部分（pass和一些细节后续会再补充一下），这里只关注平台是nv gpu的情况，如有错误，请各位大佬指正 概览由于深度学习场景的特性加之XLA中对dynamic shape并">
<meta property="og:locale">
<meta property="og:image" content="https://github.com/Ronbogo/picx-images-hosting/raw/master/screenshot-20241024-191634.9nzsfgpv0c.webp">
<meta property="og:image" content="https://github.com/Ronbogo/picx-images-hosting/raw/master/Screenshot-2024-10-24-at-16.46.47.32hyofk72n.webp">
<meta property="article:published_time" content="2024-09-27T21:40:44.000Z">
<meta property="article:modified_time" content="2024-10-24T11:36:31.365Z">
<meta property="article:author" content="Ronbogo">
<meta property="article:tag" content="AI Compiler">
<meta property="article:tag" content="XLA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/Ronbogo/picx-images-hosting/raw/master/screenshot-20241024-191634.9nzsfgpv0c.webp">
    
    
      
        
          <link rel="shortcut icon" href="/images/logo.jpg">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/logo.jpg" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/logo.jpg">
        
      
    
    <!-- title -->
    <title>torch-xla-1</title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 7.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fa-solid fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/ronbogo">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2024/09/28/buildsystem-0/"><i class="fa-solid fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2024/09/28/torch-xla-0/"><i class="fa-solid fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fa-solid fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2024/09/28/torch-xla-1/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2024/09/28/torch-xla-1/&text=torch-xla-1"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2024/09/28/torch-xla-1/&title=torch-xla-1"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2024/09/28/torch-xla-1/&is_video=false&description=torch-xla-1"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=torch-xla-1&body=Check out this article: http://example.com/2024/09/28/torch-xla-1/"><i class="fa-solid fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2024/09/28/torch-xla-1/&title=torch-xla-1"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2024/09/28/torch-xla-1/&title=torch-xla-1"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2024/09/28/torch-xla-1/&title=torch-xla-1"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2024/09/28/torch-xla-1/&title=torch-xla-1"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2024/09/28/torch-xla-1/&name=torch-xla-1&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2024/09/28/torch-xla-1/&t=torch-xla-1"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    
    
      <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Torch-XLA-%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94-%E9%9D%99%E6%80%81%E6%98%BE%E5%AD%98%E5%88%86%E9%85%8D"><span class="toc-number">1.</span> <span class="toc-text">Torch XLA 入门笔记（1）—— 静态显存分配</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%A7%88"><span class="toc-number">1.2.</span> <span class="toc-text">概览</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E8%AF%91%E6%9C%9F%E8%A7%84%E5%88%92%EF%BC%88schedule-assignment%EF%BC%89"><span class="toc-number">1.2.1.</span> <span class="toc-text">编译期规划（schedule &amp; assignment）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%BD%E8%B1%A1"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">抽象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Schedule"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Schedule</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Assignment-Bufferization"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">Assignment&#x2F;Bufferization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%9C%9F%E5%88%86%E9%85%8D%EF%BC%88allocator%EF%BC%89"><span class="toc-number">1.2.2.</span> <span class="toc-text">运行期分配（allocator）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87"><span class="toc-number">1.3.</span> <span class="toc-text">一些相关论文</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MODeL-Memory-Optimizations-for-Deep-Learning"><span class="toc-number">1.3.0.1.</span> <span class="toc-text">MODeL: Memory Optimizations for Deep Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TelaMalloc-Efficient-On-Chip-Memory-Allocation-for-Production-Machine-Learning-Accelerators"><span class="toc-number">1.3.0.2.</span> <span class="toc-text">TelaMalloc: Efficient On-Chip Memory Allocation for Production Machine Learning Accelerators</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MiniMalloc-A-Lightweight-Memory-Allocator-for-Hardware-Accelerated-Machine-Learning"><span class="toc-number">1.3.0.3.</span> <span class="toc-text">MiniMalloc: A Lightweight Memory Allocator for Hardware-Accelerated Machine Learning</span></a></li></ol></li></ol></li></ol></li></ol>
      </div>
    
  </span>
</div>

    
    <div class="content index py4 ">
        
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle p-name" itemprop="name headline">
        torch-xla-1
    </h1>



    <div class="meta">
      <span class="author p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span class="p-name" itemprop="name">Ronbogo</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2024-09-27T21:40:44.000Z" class="dt-published" itemprop="datePublished">2024-09-28</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fa-solid fa-tag"></i>
        <a class="p-category" href="/tags/AI-Compiler/" rel="tag">AI Compiler</a>, <a class="p-category" href="/tags/XLA/" rel="tag">XLA</a>
    </div>


    </div>
  </header>
  

  <div class="content e-content" itemprop="articleBody">
    <h1 id="Torch-XLA-入门笔记（1）——-静态显存分配"><a href="#Torch-XLA-入门笔记（1）——-静态显存分配" class="headerlink" title="Torch XLA 入门笔记（1）—— 静态显存分配"></a>Torch XLA 入门笔记（1）—— 静态显存分配</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本来打算第二篇写LazyTensor的，近期在忙着学校的事，还有一些事情耽误了（居然真的被催更了）并且想结合dynamo一起写一篇。便把之前写了一些的显存存货给拿出来了一部分（pass和一些细节后续会再补充一下），这里只关注平台是nv gpu的情况，如有错误，请各位大佬指正</p>
<h2 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h2><p>由于深度学习场景的特性加之XLA中对dynamic shape并不支持，整个前向与反向过程中所有tensor的shape在编译期便是已知的，因此显存的规划可以是静态的，那么我们便可以充分利用这点对宝贵的显存资源进行规划。<br>首先，静态显存规划的主要目标有二，一是通过规划降低显存峰值，使得可以使用更大的batch size或在同一硬件上跑更多的参数&#x2F;模型；二是通过规划提高中间变量的复用，以减少多次分配释放带来的时间消耗和碎片。从某种意义上来讲这个问题可以被看作是更高维度的寄存器分配问题。<br>同时，一块buffer有两个维度的属性，一个是在时间维度上的生命期，第二个是在空间维度上的大小以及offset。那么可以在一个二维坐标轴上表示这一点（如下图），可以发现这是一个2d bin packing问题，bin packing问题是一类典型的NP-hard问题，因此要优化这一类问题通常是通过启发式算法或是基于求解器（一般基于单纯形算法，感兴趣可以看看oiwiki上的介绍，dp&#x2F;搜索等这边也归入基于求解器）的算法。</p>
<p><img src="https://github.com/Ronbogo/picx-images-hosting/raw/master/screenshot-20241024-191634.9nzsfgpv0c.webp"></p>
<p>值得注意的是，如果我们可以在某一维度达到最优解或是一个足够好的解，那么我们可以将上述问题分解成两个独立问题，在XLA中便是如此。在时间维度上，XLA是一般通过一个操作序列（一个HLO instruction的序列）来表达一个计算图的（一个计算图在HLO层面是一个HLO Module，一般来讲一个HLO Module只包含一个HLO Computation），如果假设在空间维度可以达到最优，那么对于一种序列的排列，简单得将每个instruction时存活的buffer大小相加便能模拟出这种排列的峰值，而这样找到一个比较好的排列我们可以称之为schedule。</p>
<p>而在空间维度上，由于操作的顺序已经确定了，剩下的问题便是如何规划重用和offset，这部分我们可以称为assignment。</p>
<p>还需要关注的一个点是runtime的allocation情况，既然我们做了静态的显存规划，那么如果我们还像通常框架中在需要时才进行分配那就有点不合适了，所以xla runtime的分配是在计算图执行算子序列之前的，会将一个个buffer allocation按之前定下的顺序放下去，运行时的情况类似于一个内存池，每次算子是是从一块已经分配了的allocation中取一个slice作为输入&#x2F;输出显存块的。值得注意的是这些buffer allocation的总和即是显存峰值，并且他们的释放是在整个计算图执行完之后，这意味着真正的算子序列执行过程中除了少量的thread-local等的分配变化整体显存将一直保持在一个稳定值。</p>
<p>这个部分看起来非常完美但有时也有一些问题，比如对显存的可观测性影响是比较大的，不太能非常好的像torch内置的profile一样将显存的每个部分由什么使用（是activation还是gradient或者优化器参数）可视化出来对运行时显存组成进行分析，这些在做sharding相关优化时可能会有一些额外的工作量。不过这种模型与细粒度IR之间的gap造成的问题在编译中可以说是广泛存在的。</p>
<h3 id="编译期规划（schedule-assignment）"><a href="#编译期规划（schedule-assignment）" class="headerlink" title="编译期规划（schedule &amp; assignment）"></a>编译期规划（schedule &amp; assignment）</h3><h4 id="抽象"><a href="#抽象" class="headerlink" title="抽象"></a>抽象</h4><p>首先来讲一下XLA中相关的一些抽象，这块如果有一些传统的编译知识背景可能，由于笔者基础比较薄弱，这边就提一下。在assignment进行一些reuse和combine之前hlo allocation是最接近真实分配的抽象层次，如果assignment不做优化的话就是一个allocation对应一个hlo buffer, 而一个hlo buffer内部是可以有多个hlo value的（根据不同hlo instruction，比如 while instruction， 这个具体内容可以在<code>hlo_buffer.h</code>这个文件的注释里找到举例），而hlo value是SSA的，并且也是dataflow analysis和alias analysis的直接分析对象。值得注意的是内存分配的单位是hlo buffer。其实还有一些相关概念如hlo use、hlo position等，在这篇就暂时不展开了</p>
<h4 id="Schedule"><a href="#Schedule" class="headerlink" title="Schedule"></a>Schedule</h4><p>Schedule部分是以pass形式出现的，后面会以<code>LatencyHidingScheduler</code>为例，而其实pass中除了对instructions进行重排的还有一个显存优化非常相关的pass——<code>HloRematerialization</code>， 这个pass虽然名字叫rematerialization但里面其实不止有这一种优化技巧，里面有三种优化技术：compress，recompute，host_offload。想必做过MLsys特别是分布式训练的应该对他们再熟悉不过，不过这篇文章不打算在这里展开，这个话题感觉单独写一篇会更好。</p>
<p>在<code>LatencyHidingScheduler</code>这个Pass中，首先会找到有掩盖机会的computation，即存在异步的通讯操作。然后对于每一个有优化机会的computation从根部（即计算图上最后一个节点），倒着去排计算图，然后会从可选集中启发式得选最佳候选（具体规则见<code>ReadySetLt</code>类中），同时记录memory pressure。在这个过程结束后如果memory peek大于会加强memory约束重新调度数次。</p>
<h4 id="Assignment-Bufferization"><a href="#Assignment-Bufferization" class="headerlink" title="Assignment&#x2F;Bufferization"></a>Assignment&#x2F;Bufferization</h4><p>XLA中这个部分比较重点的个人认为是对buffer根据生命周期分类的处理</p>
<p>首先通过Live range分析和hlo sequence我们知道了每一块hlo buffer的生命周期和使用情况然后XLA会先把preset的分好，然后把剩余的按大小排序，从大到小，然后遇到非temp的buffer直接分，遇到temp的先尝试找之前分下去的有没有可以复用的，如果有，那么合成一个allocation，如果没有就先放着，后面统一处理。</p>
<p>然后我们就要处理temp部分了，首先这些buffer会通过heap simulator进行分配的模拟，模拟中采用两种分配策略，这里分配策略可以理解为一种像俄罗斯方块的方式，两种策略分别为size优先的贪心和lifetime优先的贪心，就是先把哪种buffr放到offset更低的位置。得到两种策略的分配结果后会取较好的那个就行combine，即将temp buffer们变成一个大的allocation在运行时一次性分配。</p>
<p>关于这部分的信息可以开启dump IR的选项来获取信息，详细可见<a target="_blank" rel="noopener" href="https://github.com/pytorch/xla/blob/master/docs/source/learn/troubleshoot.md">这里</a>。会有一个xxx-buffer-assignment的文件，里面有详细的allocation组成，buffer lifetime等</p>
<h3 id="运行期分配（allocator）"><a href="#运行期分配（allocator）" class="headerlink" title="运行期分配（allocator）"></a>运行期分配（allocator）</h3><p>运行期做的事情是将编译期规划好的，主要就是根据用户选择的分配器进行实际对allocation的分配，, 现在再NV GPU上主要选择是两种一个是GpuCudaMallocAsyncAllocator, 采用cuda memory pool和async内存操作；另一个是BFCAllocator，使用BFC策略，Doug Lea’s malloc。底层采用cuda同步api。</p>
<p>值得注意的是运行期分配是在真正的计算执行之前的，也就是计算图最开始的部分。</p>
<h2 id="一些相关论文"><a href="#一些相关论文" class="headerlink" title="一些相关论文"></a>一些相关论文</h2><p>笔者读下来当前的方向还是以基于求解器的方案为主（启发式除了上述的三种贪心的其他可能探索空间并不多），这也就导致主要的讨论在于如何抽象问题的数学建模，schedule与assignment联合求解，剪枝&#x2F;切分计算图降低编译时间等。下面大致描述一些笔者读过的论文方案与个人感受</p>
<h4 id="MODeL-Memory-Optimizations-for-Deep-Learning"><a href="#MODeL-Memory-Optimizations-for-Deep-Learning" class="headerlink" title="MODeL: Memory Optimizations for Deep Learning"></a>MODeL: Memory Optimizations for Deep Learning</h4><p>Meta的一篇工作，求解器为主，转化成ILP问题，对两个阶段都做了建模，有一些小的剪枝。但问题是模型相对还是较小。<br><img src="https://github.com/Ronbogo/picx-images-hosting/raw/master/Screenshot-2024-10-24-at-16.46.47.32hyofk72n.webp"></p>
<h4 id="TelaMalloc-Efficient-On-Chip-Memory-Allocation-for-Production-Machine-Learning-Accelerators"><a href="#TelaMalloc-Efficient-On-Chip-Memory-Allocation-for-Production-Machine-Learning-Accelerators" class="headerlink" title="TelaMalloc: Efficient On-Chip Memory Allocation for Production Machine Learning Accelerators"></a>TelaMalloc: Efficient On-Chip Memory Allocation for Production Machine Learning Accelerators</h4><p>Google的一篇工作，主要用于移动端TPU，主要是优化编译时间，论文说是集成在TPU版XLA中，以repacker形式存在（在assignment阶段的heap simulation阶段做，与原本的贪心同一位置），但我没找到源码。方法上主要是启发式与求解器结合，并有ML指导回溯。</p>
<h4 id="MiniMalloc-A-Lightweight-Memory-Allocator-for-Hardware-Accelerated-Machine-Learning"><a href="#MiniMalloc-A-Lightweight-Memory-Allocator-for-Hardware-Accelerated-Machine-Learning" class="headerlink" title="MiniMalloc: A Lightweight Memory Allocator for Hardware-Accelerated Machine Learning"></a>MiniMalloc: A Lightweight Memory Allocator for Hardware-Accelerated Machine Learning</h4><p>Google的一篇工作，通过格论剪枝，不明觉厉</p>
<hr>
<p>最近发现torch-xla那边文档重新整理了一下，也加了几篇，有需要的可以去看看</p>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
        
          <li><a href="/">Home</a></li>
        
          <li><a href="/about/">About</a></li>
        
          <li><a href="/archives/">Writing</a></li>
        
          <li><a target="_blank" rel="noopener" href="http://github.com/ronbogo">Projects</a></li>
        
      </ul>
    </div>

    
    
      <div id="toc-footer" style="display: none">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Torch-XLA-%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94-%E9%9D%99%E6%80%81%E6%98%BE%E5%AD%98%E5%88%86%E9%85%8D"><span class="toc-number">1.</span> <span class="toc-text">Torch XLA 入门笔记（1）—— 静态显存分配</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%A7%88"><span class="toc-number">1.2.</span> <span class="toc-text">概览</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E8%AF%91%E6%9C%9F%E8%A7%84%E5%88%92%EF%BC%88schedule-assignment%EF%BC%89"><span class="toc-number">1.2.1.</span> <span class="toc-text">编译期规划（schedule &amp; assignment）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8A%BD%E8%B1%A1"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">抽象</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Schedule"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Schedule</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Assignment-Bufferization"><span class="toc-number">1.2.1.3.</span> <span class="toc-text">Assignment&#x2F;Bufferization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%9C%9F%E5%88%86%E9%85%8D%EF%BC%88allocator%EF%BC%89"><span class="toc-number">1.2.2.</span> <span class="toc-text">运行期分配（allocator）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87"><span class="toc-number">1.3.</span> <span class="toc-text">一些相关论文</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MODeL-Memory-Optimizations-for-Deep-Learning"><span class="toc-number">1.3.0.1.</span> <span class="toc-text">MODeL: Memory Optimizations for Deep Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TelaMalloc-Efficient-On-Chip-Memory-Allocation-for-Production-Machine-Learning-Accelerators"><span class="toc-number">1.3.0.2.</span> <span class="toc-text">TelaMalloc: Efficient On-Chip Memory Allocation for Production Machine Learning Accelerators</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MiniMalloc-A-Lightweight-Memory-Allocator-for-Hardware-Accelerated-Machine-Learning"><span class="toc-number">1.3.0.3.</span> <span class="toc-text">MiniMalloc: A Lightweight Memory Allocator for Hardware-Accelerated Machine Learning</span></a></li></ol></li></ol></li></ol></li></ol>
      </div>
    

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2024/09/28/torch-xla-1/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2024/09/28/torch-xla-1/&text=torch-xla-1"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2024/09/28/torch-xla-1/&title=torch-xla-1"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2024/09/28/torch-xla-1/&is_video=false&description=torch-xla-1"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=torch-xla-1&body=Check out this article: http://example.com/2024/09/28/torch-xla-1/"><i class="fa-solid fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2024/09/28/torch-xla-1/&title=torch-xla-1"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2024/09/28/torch-xla-1/&title=torch-xla-1"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2024/09/28/torch-xla-1/&title=torch-xla-1"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2024/09/28/torch-xla-1/&title=torch-xla-1"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2024/09/28/torch-xla-1/&name=torch-xla-1&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2024/09/28/torch-xla-1/&t=torch-xla-1"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fa-solid fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        
          <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fa-solid fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fa-solid fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fa-solid fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2024
    Ronbogo
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/ronbogo">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script>
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="fa-regular fa-clone"></i>';
    btn += '</span>';
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
